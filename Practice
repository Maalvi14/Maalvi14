**Explain the difference between prompt injection and jailbreaking.**  
Prompt injection is an attack where malicious inputs are crafted to override or manipulate a model’s instructions. Jailbreaking is a specific type of prompt injection aimed at bypassing ethical or safety constraints, often tricking the model into providing restricted or harmful outputs.

**Is tricking ChatGPT into saying something bad necessarily prompt injection or jailbreaking? Or is it something else? Explain.**  
Not necessarily. It could be due to biases in the training data or unintended associations in the model's outputs. These are not deliberate attacks but rather an exploitation of the model’s existing weaknesses.

**Name three considerations when creating automated AI red-teaming systems. Explain one in detail (2-3 sentences).**  
1. Attack generalization  
2. Safety controls  
3. Testing robustness  

**Detail**: Testing robustness ensures that automated systems simulate realistic adversarial scenarios. This includes replicating user behavior and accounting for diverse attack types to identify vulnerabilities effectively.

**Explain why prompt injection occurs.**  
Prompt injection occurs because LLMs interpret all inputs as instructions or content to process, lacking the ability to distinguish between trusted and untrusted input sources.

**Name a few future harms that may occur from prompt hacking.**  
1. Generation of harmful content like malware or CBRNE threats.  
2. Misuse of AI in critical infrastructure.  
3. Large-scale disinformation campaigns.

**Explain why it is difficult to automate AIRT against agents, but relatively easy to do against the models that power them.**  
Automating AIRT against agents is complex due to their reliance on multi-modal and context-specific interactions, which require dynamic and adaptive attacks. Models, however, are easier targets because their behavior is primarily governed by static text-based inputs.

**Name a famous instance of prompt hacking.**  
One well-known instance is tricking OpenAI’s early models into generating malicious code or disclosing sensitive API details.

**Who first popularized prompt injection publicly?**  
Simon Willison.

**Who coined the term “prompt injection”?**  
Riley Goodside.

**What company responsibly disclosed prompt injection to OpenAI before it was publicly discovered?**  
Salesforce.

**Is AI security basically the same as classical cybersecurity?**  
No, AI security is different because it addresses vulnerabilities specific to data-driven systems, such as adversarial attacks and prompt manipulation.

**What is one important difference between AI security and classical cybersecurity?**  
AI security focuses on protecting models from adversarial inputs and prompt-based exploits, whereas classical cybersecurity defends against traditional vulnerabilities like unauthorized access or malware.

**Explain SQL injection and its relation to prompt injection.**  
SQL injection manipulates database queries by exploiting unvalidated input, similar to how prompt injection manipulates an LLM’s behavior by embedding malicious instructions.

**What is one well-known example of AI legislation?**  
The European Union's AI Act, which regulates AI systems to ensure transparency, safety, and accountability.

**Explain why fine-tuning a model on your task can prevent prompt hacking.**  
Fine-tuning aligns the model’s behavior closely with a specific task, reducing its susceptibility to untrusted or deceptive inputs.

**What does CBRNE mean? Why do companies care about it being generated by their chatbots?**  
CBRNE stands for Chemical, Biological, Radiological, Nuclear, and Explosive. Companies care because accidental generation of related instructions could lead to severe misuse or liability.

**What is RCE? Give an example of when RCE was done on a well-known math-related app.**  
RCE (Remote Code Execution) allows attackers to execute arbitrary code remotely. An example is exploiting WolframAlpha to run unintended scripts.

**What is ASR? If an attack system succeeds in 70 out of 140 attacks, what is its ASR?**  
ASR (Attack Success Rate) is the percentage of successful attacks. If 70 out of 140 attacks succeed:  
\[
\text{ASR} = \frac{70}{140} \times 100 = 50\%
\]

**What is an embodied LLM? Are they at risk of prompt hacking? Why?**  
An embodied LLM is a language model integrated into physical systems like robots. They are at risk of prompt hacking because unfiltered inputs can lead to manipulative commands affecting their real-world actions.

**What is data exfiltration?**  
Data exfiltration is the unauthorized extraction of sensitive information from a system, often using covert methods to avoid detection.

**Give an example of indirect prompt injection.**  
Embedding malicious instructions in a webpage, which an AI assistant reads and executes when summarizing or interacting with the content.

**Give an example of a multimodal prompt injection attack.**  
Displaying a QR code with encoded malicious instructions that an AI interprets when scanning the image.

**What is the context overflow attack? Explain in detail how it works.**  
A context overflow attack exploits the token limit by overloading the context window with irrelevant or adversarial content, causing critical system instructions to be truncated or ignored. This can make the model prioritize malicious inputs over original guidelines.

**Give an example of prompt leaking. Why does it matter?**  
Prompt leaking occurs when hidden system instructions are revealed through reverse-engineering outputs. It matters because these instructions can expose internal logic or sensitive information that attackers can exploit.

**Explain Sander’s three steps of thinking on the future of AIRT.**  
1. Expand attack taxonomies to include new modalities.  
2. Develop simulation environments for realistic scenarios.  
3. Establish ethical guidelines for safe red-teaming practices.

**What is a prompt template? Why is it relevant to prompt injection?**  
A prompt template is a pre-structured input format guiding an LLM’s responses. It is relevant to prompt injection because attackers can manipulate the template to alter the system's behavior or bypass constraints.

**What is artificial social intelligence and what is artificial social engineering?**  
- Artificial Social Intelligence: The ability of AI to understand and interact socially with humans.  
- Artificial Social Engineering: Manipulating AI systems to exploit vulnerabilities in human-AI interactions.

**Why do people run prompt hacking competitions?**  
Competitions simulate real-world threats, encouraging stronger defenses, innovation in red-teaming techniques, and improved AI safety practices.
